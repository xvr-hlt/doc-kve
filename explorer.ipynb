{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "\n",
    "from PIL import Image, ImageDraw\n",
    "from IPython.display import display\n",
    "\n",
    "docs = []\n",
    "\n",
    "for f in glob.glob(\"data/*.json\"):\n",
    "    docs.append(json.load(open(f)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "\n",
    "class TokFeaturiser(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, min_token_count=3, neighbourhood_size=(5, 5)):\n",
    "        self.min_token_count = min_token_count\n",
    "        self.lower_counts = None\n",
    "        self.neighbourhood_size = neighbourhood_size\n",
    "    \n",
    "    def fit(self, X, y=None, **fit_args):\n",
    "        self.lower_counts = dict(Counter(t['text'] for doc in X for t in doc['tokens']))\n",
    "        return self\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_shape(t):\n",
    "        return \"\".join(\"d\" if c.isdigit() else \"X\" if c.isupper() else \"x\" if c.islower() else c for c in t)\n",
    "\n",
    "    def _unary_featurise(self, tok):\n",
    "        bb = tok['bounding_box']\n",
    "        if bb is not None:\n",
    "            (x0,y0), (x1,y1) = bb\n",
    "            yield from zip(('x0','x1','y0','y1'), (x0,x1,y0,y1))\n",
    "            yield 'center_x', (x0+x1)/2\n",
    "            yield 'center_y', (y0+y1)/2\n",
    "            yield 'width', x1-x0\n",
    "            yield 'height', y1-y0\n",
    "\n",
    "        txt = tok['text']\n",
    "        if self.lower_counts.get(txt.lower, -1) > self.min_token_count:\n",
    "            yield 'text', txt\n",
    "            yield 'text_lower', txt.lower()\n",
    "            yield 'text[:3]', txt[:3]\n",
    "            yield 'text[-3:]', text[-3:]\n",
    "        \n",
    "        shape = self._get_shape(txt)\n",
    "        yield 'shape', shape\n",
    "        yield 'shape_lower', shape.lower()\n",
    "        yield 'shape[:3]', shape[:3]\n",
    "        yield 'shape[-3:]', shape[-3:]\n",
    "        \n",
    "    def _add_window_features(self, doc):\n",
    "        Xt = []\n",
    "        doc_copy = [dict(f) for f in doc]\n",
    "        left_window, right_window = self.neighbourhood_size\n",
    "        for i, feats in enumerate(doc):\n",
    "            left_nhood = doc_copy[max(0, i-left_window):i]\n",
    "            for offset_ix, neighbour_feat in enumerate(reversed(left_nhood), 1):\n",
    "                feats.update({f'wndw-{offset_ix}::{k}':v for k,v in neighbour_feat.items()})\n",
    "\n",
    "            right_nhood = doc_copy[i+1:i+right_window]\n",
    "            for offset_ix, neighbour_feat in enumerate(right_nhood, 1):\n",
    "                feats.update({f'wndw+{offset_ix}::{k}':v for k,v in neighbour_feat.items()})\n",
    "            Xt.append(feats)\n",
    "        return Xt\n",
    "        \n",
    "        \n",
    "    def transform(self, X):\n",
    "        X = [dict(x) for x in X]\n",
    "        Xt = []\n",
    "        for doc in X:\n",
    "            Xt.append([dict(self._unary_featurise(t)) for t in doc['tokens']])\n",
    "        Xt = [self._add_window_features(doc) for doc in Xt]\n",
    "        for x, f in zip(X, Xt):\n",
    "            x['features'] = f\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "class InstanceGenerator(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, field_percentile=95, table_percentile=90):\n",
    "        self.field_percentile = field_percentile\n",
    "        self.table_percentile = table_percentile\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_percentile_window(label_values, percentile):\n",
    "        left_lengths, right_lengths = [], []\n",
    "        for label, value in label_values:\n",
    "            if not label:\n",
    "                continue\n",
    "            if not value:\n",
    "                left_lengths.append(0), right_lengths.append(0)\n",
    "                continue\n",
    "            left_lengths.append(max(0, min(label) - min(value)))\n",
    "            right_lengths.append(max(0, max(value) - max(label)))\n",
    "        return int(np.percentile(left_lengths, percentile)+1), int(np.percentile(right_lengths, percentile)+1)\n",
    "\n",
    "\n",
    "    def fit(self, X, y=None, **fit_args):\n",
    "        field_label_values = [(f['label'], f['value']) for annot in y for f in annot['field']]        \n",
    "        table_label_values = [[column['label'], val] for annot in y\n",
    "                                  for table in annot['table']\n",
    "                                  for column in table['columns']\n",
    "                                  for val in column['value']]\n",
    "        self.field_window = self.get_percentile_window(field_label_values, self.field_percentile)\n",
    "        self.table_window = self.get_percentile_window(table_label_values, self.table_percentile)\n",
    "        return self\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_pairwise_features(a_features, b_features):\n",
    "        features = {}\n",
    "        for fname, feature in (('a', a_features),  ('b', b_features)):\n",
    "            features.update({\n",
    "                f'pairwise::{fname}[0]::{k}': v for k,v in feature[0].items()\n",
    "            })\n",
    "            if len(feature) > 1:\n",
    "                features.update({\n",
    "                    f'pairwise::{fname}[-1]::{k}': v for k,v in feature[-1].items()\n",
    "                })\n",
    "        return features\n",
    "    \n",
    "    def get_positional_features(a_ixs, b_ixs, tokens):\n",
    "        pass\n",
    "        \n",
    "    \n",
    "    def get_doc_candidates(self, values, value_type, doc, window):\n",
    "        instances = []\n",
    "        w_left, w_right = window\n",
    "        for v in values:\n",
    "            left_candidates = range(max(0, v[0]-w_left), v[0])\n",
    "            rl = min(v[-1]+1, len(doc['features']) - 1)\n",
    "            rr = min(v[-1]+1+w_right, len(doc['features']))\n",
    "            right_candidates = range(rl, rr)\n",
    "            \n",
    "            for candidate in chain(left_candidates, right_candidates):\n",
    "                features = self.get_pairwise_features(\n",
    "                    [doc['features'][ix] for ix in v],\n",
    "                    [doc['features'][candidate]]\n",
    "                )\n",
    "                features.update({'type': value_type})\n",
    "                instances.append({\n",
    "                    'doc_id': doc['doc_id'],\n",
    "                    'key': v,\n",
    "                    'value': candidate,\n",
    "                    'features': features,\n",
    "                    'type': value_type\n",
    "                })\n",
    "        return instances\n",
    "        \n",
    "    def transform(self, X):\n",
    "        X = [dict(x) for x in X]\n",
    "        for doc in X:\n",
    "            instances = []\n",
    "            instances += self.get_doc_candidates(doc['fields'], 'field', doc, self.field_window)\n",
    "            instances += self.get_doc_candidates(doc['tables'], 'table', doc, self.table_window)\n",
    "            doc['instances'] = instances\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm.sklearn import LGBMClassifier\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class InstanceModel(LGBMClassifier):\n",
    "    def fit(self, X, y=None, **fit_args):\n",
    "        flat_X, flat_y = [], []\n",
    "        for xi, yi in zip(X, y):\n",
    "            label_map = {tuple(f['label']): set(f['value']) for f in yi['field'] if f['label']}\n",
    "            label_map.update({tuple(c['label']):set(c for cv in c['value'] for c in cv) for t in yi['table'] for c in t['columns']})\n",
    "            for i in xi['instances']:\n",
    "                flat_X.append(i['features'])\n",
    "                flat_y.append(i['value'] in label_map[tuple(i['key'])])\n",
    "        self.vectorizer = DictVectorizer()\n",
    "        flat_X = self.vectorizer.fit_transform(flat_X)\n",
    "        return super().fit(flat_X, flat_y, **fit_args)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = [dict(x) for x in X]\n",
    "        features = []\n",
    "        lens = []\n",
    "        preds = []\n",
    "        for doc in X:\n",
    "            xf = [i['features'] for i in doc['instances']]\n",
    "            lens.append(len(xf))\n",
    "            features += xf\n",
    "        features = self.vectorizer.transform(features)\n",
    "        flat_preds = super().predict(features)\n",
    "        \n",
    "        offset = 0\n",
    "        for doc, nfeats in zip(X, lens):\n",
    "            values = {'field': {}, 'table': {}}\n",
    "            for instance, pred in zip(doc['instances'], flat_preds[offset:offset+nfeats]):\n",
    "                k, v = tuple(instance['key']), instance['value']\n",
    "                if k not in values[instance['type']]:\n",
    "                    values[instance['type']][k] = set()\n",
    "                if pred:\n",
    "                    values[instance['type']][k].add(v)\n",
    "            offset += nfeats\n",
    "            preds.append(values)\n",
    "        return preds\n",
    "    \n",
    "    @staticmethod\n",
    "    def evaluate(ytrue, ypred):\n",
    "        ent_types = ('field', 'table')\n",
    "        results = []\n",
    "        for yt, yp in zip(ytrue, ypred):\n",
    "            label_map = {tuple(f['label']): set(f['value']) for f in yt['field'] if f['label']}\n",
    "            label_map.update({tuple(c['label']):set(c for cv in c['value'] for c in cv) for t in yt['table'] for c in t['columns']})\n",
    "            for t in ent_types:\n",
    "                for k, pred_v in yp[t].items():\n",
    "                    true_v = label_map[k]\n",
    "                    results.append({\n",
    "                        'type': t,\n",
    "                        'k': k,\n",
    "                        'pred_v': pred_v,\n",
    "                        'true_v': true_v,\n",
    "                        'iou': len(pred_v & true_v)/len(pred_v | true_v) if pred_v else int(pred_v == true_v),\n",
    "                        'match': pred_v == true_v\n",
    "                    })\n",
    "\n",
    "        metrics = {}\n",
    "        for met in ('iou', 'match'):\n",
    "            for t in ('field', 'table'):\n",
    "                metrics[f'{t}.{met}'] = np.mean([r[met] for r in results if r['type'] == t])\n",
    "            metrics[f'micro.{met}'] = np.mean([r[met] for r in results])\n",
    "        return metrics, results\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        metrics, results = self.evaluate(y, self.predict(X))\n",
    "        return metrics['micro.iou']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = [{\n",
    "    'tokens': d['tokens'],\n",
    "    'fields': [f['label'] for f in d['annotations']['field'] if f['label']],\n",
    "    'tables': [c['label'] for table in d['annotations']['table'] for c in table['columns'] if c['label']],\n",
    "    'doc_id': d['doc_id']\n",
    "} for d in docs]\n",
    "\n",
    "y = [doc['annotations'] for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline((\n",
    "    ('featuriser', TokFeaturiser()),\n",
    "    ('instance_generator', InstanceGenerator()),\n",
    "    ('model', InstanceModel(verbose=1, n_jobs=-1))    \n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = pipeline.fit(X,y).predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics, results = InstanceModel.evaluate(y, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'field.iou': 0.5774592952558371,\n",
       " 'table.iou': 0.3959119155673821,\n",
       " 'micro.iou': 0.536086252945883,\n",
       " 'field.match': 0.4977973568281938,\n",
       " 'table.match': 0.26119402985074625,\n",
       " 'micro.match': 0.44387755102040816}"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'featuriser__min_token_count': [3,10],\n",
    "    'featuriser__neighbourhood_size': [(3,3), (5,5), (10,10)],\n",
    "    'instance_generator__field_percentile': [95,99,100],\n",
    "    'instance_generator__table_percentile': [90, 95]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "grid = GridSearchCV(pipeline, param_grid, verbose=True, cv=3, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "grid.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_coords(token, width, height):\n",
    "    (x1, y1), (x2, y2) = token[\"bounding_box\"]                        \n",
    "    x1, y1, x2, y2 = int(round(x1*width)), int(round(y1*height)), int(round(x2*width)), int(round(y2*height))\n",
    "    return ((x1, y1), (x2, y2))\n",
    "\n",
    "def show(doc):\n",
    "    image = Image.open(\"data/{}.jpg\".format(doc[\"doc_id\"]))\n",
    "    width, height = image.width, image.height\n",
    "    \n",
    "    annotations = doc[\"annotations\"]\n",
    "    tokens = doc[\"tokens\"]\n",
    "    \n",
    "    overlay = Image.new(\"RGBA\", image.size, (255, 255, 255, 0))\n",
    "    draw = ImageDraw.Draw(overlay)\n",
    "\n",
    "    for field in annotations[\"field\"]:\n",
    "        for token_idx in field[\"label\"]:\n",
    "            token = tokens[token_idx]\n",
    "            draw.rectangle(token_coords(token, width, height), fill=(0, 255, 0, 100))\n",
    "        for token_idx in field[\"value\"]:\n",
    "            token = tokens[token_idx]\n",
    "            draw.rectangle(token_coords(token, width, height), fill=(0, 0, 255, 100))\n",
    "    for table in annotations[\"table\"]:\n",
    "        for column in table[\"columns\"]:\n",
    "            for token_idx in column[\"label\"]:\n",
    "                token = tokens[token_idx]\n",
    "                draw.rectangle(token_coords(token, width, height), fill=(255, 255, 0, 100))\n",
    "            for row in column[\"value\"]:\n",
    "                for token_idx in row:\n",
    "                    token = tokens[token_idx]\n",
    "                    draw.rectangle(token_coords(token, width, height), fill=(255, 0, 255, 100))\n",
    "\n",
    "    image = image.convert(\"RGBA\")\n",
    "    image = Image.alpha_composite(image, overlay)\n",
    "    display(image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
